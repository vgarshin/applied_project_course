{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cd1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_PATH = '../../__STS'\n",
    "\n",
    "RAW_DATA_PATH_BLACK = 'raw_data_black'\n",
    "all_black_htmls = os.listdir(f'{WORK_PATH}/{RAW_DATA_PATH_BLACK}')\n",
    "print(len(all_black_htmls))\n",
    "print(all_black_htmls[:10])\n",
    "\n",
    "RAW_DATA_PATH_WHITE = 'raw_data_white'\n",
    "all_white_htmls = os.listdir(f'{WORK_PATH}/{RAW_DATA_PATH_WHITE}')\n",
    "print(len(all_white_htmls))\n",
    "print(all_white_htmls[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd044ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(html):\n",
    "    \"\"\"\n",
    "    Extracts just text from the sites\n",
    "    Text is enough for sites classificatio\n",
    "    You do not need to look at bad pictures...\n",
    "    \n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    for script in soup(['script', 'style']):\n",
    "        script.extract()\n",
    "    page_text = soup.get_text()\n",
    "    for ch in ['\\n', '\\t', '\\r']:\n",
    "        page_text = page_text.replace(ch, ' ')\n",
    "    return ' '.join(page_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_texts(all_htmls, path):\n",
    "    \"\"\"\n",
    "    Loop for data processing, \n",
    "    we process only files of loaded htmls\n",
    "    \n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for html_file in tqdm(all_htmls):\n",
    "        try:\n",
    "            data_dict = {}\n",
    "            with open(f'{path}/{html_file}', 'r', encoding='utf-8') as file:\n",
    "                html = file.read()\n",
    "            text = get_text(html)\n",
    "            data_dict['site_name'] = html_file.replace('.html', '')\n",
    "            data_dict['text'] = text\n",
    "            data_dict['language'] = detect(text) if text else 'unknown'\n",
    "            data_list.append(data_dict)\n",
    "        except Exception as e:\n",
    "            print(html_file, '| error |', e)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = collect_texts(\n",
    "    all_white_htmls, \n",
    "    f'{WORK_PATH}/{RAW_DATA_PATH_WHITE}'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a478d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame(data_list)\n",
    "print(df_data.shape)\n",
    "display(df_data.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "native-pennsylvania",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.groupby(['language'])['language'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee206dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2 as pm\n",
    "import nltk\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "N_CORES = min(\n",
    "    multiprocessing.cpu_count(), \n",
    "    int(float(os.environ['CPU_LIMIT']))\n",
    ")\n",
    "print('cores:', N_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eab519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process only english for start\n",
    "df_data_en = df_data[df_data.language == 'en']\n",
    "df_data_en.reset_index(inplace=True)\n",
    "del df_data_en['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = 'english' # 'russian' or 'english'\n",
    "MORPH = pm.MorphAnalyzer()\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = nltk.corpus.stopwords.words(LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence, as_list=False):\n",
    "    s = re.sub('[^а-яА-Яa-zA-Z]+', ' ', sentence).strip().lower()\n",
    "    s = re.sub('ё', 'е', s)\n",
    "    funсtion_words = {'INTJ', 'PRCL', 'CONJ', 'PREP'}\n",
    "    lemmatized_words = list(map(lambda word: MORPH.parse(word)[0], s.split()))\n",
    "    result = []\n",
    "    for word in lemmatized_words:\n",
    "        if word.tag.POS not in funсtion_words:\n",
    "            result.append(word.normal_form)\n",
    "    result = [w for w in result if w not in STOPWORDS]\n",
    "    if as_list:\n",
    "        return result\n",
    "    else:\n",
    "        return ' '.join(result)\n",
    "    \n",
    "def apply_parallel(texts, func, n_cores=2):\n",
    "    pool = Pool(n_cores)\n",
    "    split = np.array_split(texts, n_cores)\n",
    "    res = [item for sub in pool.map(func, split) for item in sub]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return res\n",
    "\n",
    "def preprocessing_list(sentences):\n",
    "    return [preprocessing(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "proc = apply_parallel(df_data_en.text, preprocessing_list, n_cores=N_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9e7d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_data_en.loc[:, 'proc'] = proc\n",
    "print(df_data_en.shape)\n",
    "display(df_data_en.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_en[['site_name', 'proc']].to_csv('data_white_en.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-market",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.read_csv('data_white_en.csv')\n",
    "print(df_tmp.shape)\n",
    "display(df_tmp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-sense",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incoming-reduction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253850a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
