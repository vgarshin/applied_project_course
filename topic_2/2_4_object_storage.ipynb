{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Project in Big Data on Industrial Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA COLLECTION TECHNIQUES\n",
    "## Part IV. Load from object storage and preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries and credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[About boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        access_data = json.load(file)\n",
    "    return access_data\n",
    "\n",
    "creds = access_data(file_path='access_bucket.json')\n",
    "print(creds.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Session and client for loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "s3 = session.client(\n",
    "    service_name='s3',\n",
    "    aws_access_key_id=creds['aws_access_key_id'],\n",
    "    aws_secret_access_key=creds['aws_secret_access_key'],\n",
    "    endpoint_url='https://storage.yandexcloud.net'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VK_DATA_BUCKET = 'apid-data-vk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [key['Key'] for key in s3.list_objects(Bucket=VK_DATA_BUCKET)['Contents']]\n",
    "print('files in storage:', all_files[:10]) # works only for num of files < 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load data from the storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_load = all_files[1]\n",
    "print('file to load:', file_to_load)\n",
    "get_object_response = s3.get_object(\n",
    "    Bucket=VK_DATA_BUCKET, \n",
    "    Key=file_to_load\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_object_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(get_object_response['Body'])\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Time for Big Data (Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, struct, count_distinct, from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()\n",
    "conf.set('spark.master', 'local[5]')\n",
    "conf.set('spark.executor.memory', '4G')\n",
    "conf.set('spark.driver.memory', '4G')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.access.key', creds['aws_access_key_id'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.secret.key', creds['aws_secret_access_key'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.impl','org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.multipart.size', '104857600')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.block.size', '33554432')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.threads.max', '256')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 'http://storage.yandexcloud.net')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = file_path = f's3a://{VK_DATA_BUCKET}/' + all_files[3]\n",
    "sdf = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(F.explode(sdf.items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_df(df, prefix=None):\n",
    "    flat_cols = [c[0] for c in df.dtypes if c[1][:6] != 'struct']\n",
    "    nested_cols = [c[0] for c in df.dtypes if c[1][:6] == 'struct']\n",
    "    flat_df = df.select(\n",
    "        flat_cols + \n",
    "        [F.col(ncol + '.' + col).alias(prefix + col if prefix else ncol + '_' + col ) \n",
    "         for ncol in nested_cols \n",
    "         for col in df.select(ncol + '.*').columns]\n",
    "    )\n",
    "    return flat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = flat_df(sdf, prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can load ALL files with posts from ALL walls and users\n",
    "file_path = file_path = f's3a://{VK_DATA_BUCKET}/gsom_ma-2022-11-14-05-00-20-220713/walls/wall_owner_id_*.json'\n",
    "sdf = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf.select(F.explode(sdf.groups))\n",
    "sdf = flat_df(sdf, prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Data processing and basic EDA - VK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = [\n",
    "    f's3a://{VK_DATA_BUCKET}/gsom_spbu-2022-11-16-05-00-11-755938/wall_owner_id_*.json',\n",
    "    f's3a://{VK_DATA_BUCKET}/gsom_ma-2022-11-14-05-00-20-220713/wall_owner_id_*.json',\n",
    "    f's3a://{VK_DATA_BUCKET}/gsom_abiturient-2022-11-15-05-00-12-366823//wall_owner_id_*.json'\n",
    "]\n",
    "sdf = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataset v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1 = sdf.select(F.explode(sdf.groups), sdf.items)\n",
    "sdf1 = flat_df(sdf1, prefix='')\n",
    "sdf1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1 = sdf1.select(sdf1.col_id, sdf1.col_name, F.explode(sdf.items))\n",
    "sdf1 = flat_df(sdf1, prefix='')\n",
    "sdf1.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1.groupBy(\"col_name\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dataset v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_groups = sdf.select(F.explode(sdf.groups))\n",
    "sdf_groups = flat_df(sdf_groups, prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_groups.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts = sdf.select(F.explode(sdf.items))\n",
    "sdf_posts = flat_df(sdf_posts, prefix='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sdf_posts.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts = sdf_posts.withColumn('col_owner_id', F.regexp_replace('col_owner_id', '-', ''))\n",
    "sdf_posts = sdf_posts.withColumn('col_from_id', F.regexp_replace('col_from_id', '-', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts = sdf_posts.join(sdf_groups, sdf_posts.col_owner_id == sdf_groups.col_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.groupBy(\"col_name\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.select('col_name').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts.filter(sdf_posts.col_name == 'Высшая школа менеджмента СПбГУ').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_posts \\\n",
    "    .filter(sdf_posts.col_name == 'Высшая школа менеджмента СПбГУ') \\\n",
    "    .limit(5) \\\n",
    "    .toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf_posts.select(\n",
    "    sdf_posts.col_date,\n",
    "    sdf_posts.col_comments,\n",
    "    sdf_posts.col_likes,\n",
    "    sdf_posts.col_reposts,\n",
    "    sdf_posts.col_views,\n",
    "    sdf_posts.col_text,\n",
    "    sdf_posts.col_name,\n",
    "    sdf_posts.col_screen_name\n",
    ").dropDuplicates()\n",
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.groupBy(\"col_name\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Data processing and basic EDA - AI Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './ai_jobs_data/*.json'\n",
    "sdf = spark.read.json(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.employmentType.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
