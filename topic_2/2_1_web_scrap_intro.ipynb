{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "simple-pilot",
   "metadata": {},
   "source": [
    "# Applied Project in Big Data on Industrial Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loved-giant",
   "metadata": {},
   "source": [
    "## DATA COLLECTION TECHNIQUES\n",
    "## Part I. Web scraping intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-wisdom",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-rubber",
   "metadata": {},
   "source": [
    "Let's start from very basic example, we wiil need [urllib](https://docs.python.org/3/library/urllib.html) library for  opening and reading URLs. We will also use [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) Python library to parce HTML data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conceptual-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-directive",
   "metadata": {},
   "source": [
    "### 2. Get text from HTML page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_2_SCRAP = 'https://ai-jobs.net'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = Request(URL_2_SCRAP)\n",
    "response = urlopen(request)\n",
    "html = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa184175",
   "metadata": {},
   "outputs": [],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bb0223",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b4f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-story",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accurate-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = soup.text\n",
    "for ch in ['\\n', '\\t', '\\r']:\n",
    "    text = text.replace(ch, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confidential-indianapolis",
   "metadata": {},
   "source": [
    "### 3. Simple NLP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-cricket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower text and leave only text without symbols\n",
    "text = re.sub('[^а-яА-Яa-zA-Z]+', ' ', text).strip().lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-forth",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_list = text.split()\n",
    "text_as_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-values",
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(text_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = dict(Counter(text_as_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = dict(\n",
    "    sorted(\n",
    "        freqs.items(), \n",
    "        key=lambda item: item[1], \n",
    "        reverse=True\n",
    "    )\n",
    ")\n",
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acknowledged-brooks",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_bar = {k: v for k, v in freqs.items() if v >= 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "plt.bar(*zip(*freqs_bar.items()))\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-bargain",
   "metadata": {},
   "source": [
    "### 4. Save HTML to disk as row data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-license",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ai_jobs_page.html', 'w') as file:\n",
    "    file.write(html.decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latter-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ai_jobs_page.html', 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-investor",
   "metadata": {},
   "source": [
    "### 5. More than text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-granny",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('img', attrs={'alt': 'Oak Ridge National Laboratory logo'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('img', attrs={'alt': re.compile(r\".*Gorovoy\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-dimension",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('img')[19]['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_2_SCRAP_IMG = 'https://ai-jobs.net/' + soup.find_all('img')[19]['src']\n",
    "print(URL_2_SCRAP_IMG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = Request(URL_2_SCRAP_IMG)\n",
    "response = urlopen(request)\n",
    "img = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-columbia",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "img = Image.open(img)\n",
    "plt.imshow(np.array(img))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.save('logo.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11536061",
   "metadata": {},
   "source": [
    "### 6. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66895e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01668ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07dd711",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baba71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a', class_=\"col list-group-item-action px-2 py-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abfd418",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_list = soup.find_all('a', class_=\"col list-group-item-action px-2 py-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613865ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd35ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373f2353",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_list[1]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5bf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_list[1]['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419c6f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_list[1].find_all('span')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca7e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_list[1].find_all('span')[0]['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b401b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_list[1].find('span', class_='d-block d-md-none text-break job-list-item-location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_list[1].find_all('span', class_='badge badge-light badge-pill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3163dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x.text for x in jobs_list[1].find_all('span', class_='badge badge-light badge-pill')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8abd1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in jobs_list:\n",
    "    print(job['title'])\n",
    "    print(URL_2_SCRAP + job['href'])\n",
    "    print(\n",
    "        'location ->',\n",
    "        job.find('span', class_='d-block d-md-none text-break job-list-item-location').text\n",
    "    )\n",
    "    print(\n",
    "        'requirements ->',\n",
    "        [x.text for x in job.find_all('span', class_='badge badge-light badge-pill')]\n",
    "    )\n",
    "    print('*' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13035501",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_jobs = []\n",
    "for job in jobs_list:\n",
    "    job_dict = {}\n",
    "    job_dict['description'] = job['title']\n",
    "    job_dict['url'] = URL_2_SCRAP + job['href']\n",
    "    location = job.find(\n",
    "        'span', \n",
    "        class_='d-block d-md-none text-break job-list-item-location'\n",
    "    )\n",
    "    job_dict['location'] = location.text if location else ''\n",
    "    time = job.find(\n",
    "        'span', \n",
    "        class_='badge badge-secondary badge-pill my-md-1'\n",
    "    )\n",
    "    job_dict['time'] = time.text if time else ''\n",
    "    position = job.find(\n",
    "        'span', \n",
    "        class_='badge badge-info badge-pill my-md-1 d-md-none'\n",
    "    )\n",
    "    job_dict['position'] = position.text if position else ''\n",
    "    level = job.find(\n",
    "        'span', \n",
    "        class_='badge badge-info badge-pill my-md-1 d-none d-md-inline-block'\n",
    "    )\n",
    "    job_dict['level'] = level.text if level else ''\n",
    "    salary_range = job.find(\n",
    "        'span', \n",
    "        class_='badge badge-success badge-pill d-none d-md-inline-block'\n",
    "    )\n",
    "    job_dict['salary_range'] = salary_range.text if salary_range else ''\n",
    "    salary = job.find(\n",
    "        'span', \n",
    "        class_='badge badge-success badge-pill d-md-none'\n",
    "    )\n",
    "    job_dict['salary'] = salary.text if salary else ''\n",
    "    job_dict['requirements'] = [\n",
    "        x.text for x in job.find_all('span', class_='badge badge-light badge-pill')\n",
    "    ]\n",
    "    job_dict['offers'] = [\n",
    "        x.text for x in job.find_all('span', class_='badge badge-success badge-pill')\n",
    "    ]\n",
    "    all_jobs.append(job_dict)\n",
    "all_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0c6880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eeff98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(all_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d92a93",
   "metadata": {},
   "source": [
    "### 7. Get data by single item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dcd9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_2_SCRAP = 'https://ai-jobs.net/job/32218-data-science-content-intern-remote/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650c77d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = Request(URL_2_SCRAP)\n",
    "response = urlopen(request)\n",
    "html = response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc682a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ba5108",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2306139",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('script', type=\"application/ld+json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d1025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cac98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = soup.find('script', type=\"application/ld+json\").text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(text)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['baseSalary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c0366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['identifier']['value']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334d1331",
   "metadata": {},
   "source": [
    "### 8. Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ebd5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'ai_jobs_data'\n",
    "os.makedirs(folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993eff1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in all_jobs[:3]:\n",
    "    print(job['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42131ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c18ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in tqdm(all_jobs):\n",
    "    request = Request(job['url'])\n",
    "    response = urlopen(request)\n",
    "    html = response.read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    text = soup.find('script', type=\"application/ld+json\").text\n",
    "    data = json.loads(text, strict=False)\n",
    "   \n",
    "    job_id =  data['identifier']['value']\n",
    "    file_path = f'{folder}/{job_id}.json'\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file)\n",
    "    \n",
    "    time.sleep(.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e116bb71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
