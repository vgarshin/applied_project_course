{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "considerable-contemporary",
   "metadata": {},
   "source": [
    "# Applied Project in Big Data on Industrial Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-stomach",
   "metadata": {},
   "source": [
    "## DATA PROCESSING TECHNIQUES\n",
    "## Part III. Processing all the collected data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manufactured-kelly",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from random import randint\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-wales",
   "metadata": {},
   "source": [
    "### 2. Collect saved files to one dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cd1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_PATH = '.'\n",
    "RAW_DATA_PATH = 'data/blacklist-HTTP-only.txt'\n",
    "all_black_htmls = os.listdir(f'{WORK_PATH}/{RAW_DATA_PATH}')\n",
    "print(len(all_black_htmls))\n",
    "print(all_black_htmls[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd044ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(html):\n",
    "    \"\"\"\n",
    "    Extracts just text from the sites\n",
    "    Text is enough for sites classificatio\n",
    "    You do not need to look at bad pictures...\n",
    "    \n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    for script in soup(['script', 'style']):\n",
    "        script.extract()\n",
    "    page_text = soup.get_text()\n",
    "    for ch in ['\\n', '\\t', '\\r']:\n",
    "        page_text = page_text.replace(ch, ' ')\n",
    "    return ' '.join(page_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop for data processing, we process only files of loaded htmls\n",
    "\n",
    "data_list = []\n",
    "for html_file in tqdm(all_black_htmls):\n",
    "    data_dict = {}\n",
    "    with open(f'{WORK_PATH}/{RAW_DATA_PATH}/{html_file}', 'r', encoding='utf-8') as file:\n",
    "        html = file.read()\n",
    "    text = get_text(html)\n",
    "    data_dict['site_name'] = html_file.replace('.html', '')\n",
    "    data_dict['text'] = text\n",
    "    data_dict['language'] = detect(text) if text else 'unknown'\n",
    "    data_list.append(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066d48cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a478d747",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.DataFrame(data_list)\n",
    "print(df_data.shape)\n",
    "display(df_data.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-grounds",
   "metadata": {},
   "source": [
    "### 3. Parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee206dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2 as pm\n",
    "import nltk\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "N_CORES = min(\n",
    "    multiprocessing.cpu_count(), \n",
    "    int(float(os.environ['CPU_LIMIT']))\n",
    ")\n",
    "print('cores:', N_CORES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eab519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process only english for start\n",
    "df_data_en = df_data[df_data.language == 'en']\n",
    "df_data_en.reset_index(inplace=True)\n",
    "del df_data_en['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043c9534",
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = 'english' # 'russian' or 'english'\n",
    "MORPH = pm.MorphAnalyzer()\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = nltk.corpus.stopwords.words(LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(sentence, as_list=False):\n",
    "    s = re.sub('[^а-яА-Яa-zA-Z]+', ' ', sentence).strip().lower()\n",
    "    s = re.sub('ё', 'е', s)\n",
    "    funсtion_words = {'INTJ', 'PRCL', 'CONJ', 'PREP'}\n",
    "    lemmatized_words = list(map(lambda word: MORPH.parse(word)[0], s.split()))\n",
    "    result = []\n",
    "    for word in lemmatized_words:\n",
    "        if word.tag.POS not in funсtion_words:\n",
    "            result.append(word.normal_form)\n",
    "    result = [w for w in result if w not in STOPWORDS]\n",
    "    if as_list:\n",
    "        return result\n",
    "    else:\n",
    "        return ' '.join(result)\n",
    "    \n",
    "def apply_parallel(texts, func, n_cores=2):\n",
    "    pool = Pool(n_cores)\n",
    "    split = np.array_split(texts, n_cores)\n",
    "    res = [item for sub in pool.map(func, split) for item in sub]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return res\n",
    "\n",
    "def preprocessing_list(sentences):\n",
    "    return [preprocessing(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "proc = apply_parallel(df_data_en.text, preprocessing_list, n_cores=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b9e7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_en.loc[:, 'proc'] = proc\n",
    "print(df_data_en.shape)\n",
    "display(df_data_en.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-induction",
   "metadata": {},
   "source": [
    "### 4. Example of features based on text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173caf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f6e15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DF = .7\n",
    "MIN_DF = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec1dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector_text(df, col, tfidf=True, max_df=MAX_DF , min_df=MIN_DF):\n",
    "    tfidf_text = []\n",
    "    for i, row in df.iterrows():\n",
    "        tfidf_text.append(row[col])\n",
    "    print(len(tfidf_text))\n",
    "    if tfidf:\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            ngram_range=(1, 1), \n",
    "            max_df=max_df, \n",
    "            min_df=min_df\n",
    "        )\n",
    "    else:\n",
    "        vectorizer = CountVectorizer(\n",
    "            ngram_range=(1, 1), \n",
    "            max_df=max_df, \n",
    "            min_df=min_df\n",
    "        )\n",
    "    features = vectorizer.fit_transform(tfidf_text)\n",
    "    print(features.shape, np.max(features), np.min(features))\n",
    "    result = pd.concat(\n",
    "        [\n",
    "            df,\n",
    "            pd.DataFrame(\n",
    "                features.todense(), \n",
    "                columns=vectorizer.get_feature_names()\n",
    "            )\n",
    "        ], \n",
    "        axis=1\n",
    "    )\n",
    "    return result, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_data_en, vectorizer = get_vector_text(\n",
    "    df_data_en, \n",
    "    col='proc', \n",
    "    tfidf=True,\n",
    "    max_df=MAX_DF, \n",
    "    min_df=MIN_DF\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215725c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_data_en.shape)\n",
    "df_data_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253850a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
