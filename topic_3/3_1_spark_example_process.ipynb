{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Project in Big Data on Industrial Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PROCESSING TECHNIQUES\n",
    "## Part I. Use of Spark to create dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def access_data(file_path):\n",
    "    with open(file_path) as file:\n",
    "        access_data = json.load(file)\n",
    "    return access_data\n",
    "\n",
    "creds = access_data(file_path='access_bucket.json')\n",
    "print(creds.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Browse files at S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = boto3.session.Session()\n",
    "s3 = session.client(\n",
    "    service_name='s3',\n",
    "    aws_access_key_id=creds['aws_access_key_id'],\n",
    "    aws_secret_access_key=creds['aws_secret_access_key'],\n",
    "    endpoint_url='https://storage.yandexcloud.net'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTS_DATA_BUCKET = 'apid-data-options'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [key['Key'] for key in s3.list_objects(Bucket=OPTS_DATA_BUCKET)['Contents']]\n",
    "print('files in storage:', all_files[:10]) # works only for num of files < 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Spark processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, struct, count_distinct, from_unixtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uiWebUrl(self):\n",
    "    from urllib.parse import urlparse\n",
    "    web_url = self._jsc.sc().uiWebUrl().get()\n",
    "    port = urlparse(web_url).port\n",
    "    return '{}proxy/{}/jobs/'.format(os.environ['JUPYTERHUB_SERVICE_PREFIX'], port)\n",
    "\n",
    "SparkContext.uiWebUrl = property(uiWebUrl)\n",
    "\n",
    "conf = SparkConf()\n",
    "conf.set('spark.master', 'local[*]')\n",
    "conf.set('spark.executor.memory', '24G')\n",
    "conf.set('spark.driver.memory', '8G')\n",
    "conf.set('spark.driver.maxResultSize', '4G')\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.access.key', creds['aws_access_key_id'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.secret.key', creds['aws_secret_access_key'])\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.impl','org.apache.hadoop.fs.s3a.S3AFileSystem')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.multipart.size', '104857600')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.block.size', '33554432')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.threads.max', '256')\n",
    "spark._jsc.hadoopConfiguration().set('fs.s3a.endpoint', 'http://storage.yandexcloud.net')\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Read base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 1 year for the start\n",
    "file_path = f's3a://{OPTS_DATA_BUCKET}/' + 'data/L3_options_2017*.parquet'\n",
    "options = spark.read.parquet(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = options.agg({'date': 'min'}).collect()[0].asDict()['min(date)']\n",
    "max_date = options.agg({'date': 'max'}).collect()[0].asDict()['max(date)']\n",
    "print('from', min_date, 'to', max_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Filter by assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_count = (\n",
    "    options\n",
    "        .groupBy('base_symbol')\n",
    "        .count()\n",
    "        .orderBy('count', ascending=False)\n",
    ")\n",
    "assets_count.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_selected = assets_count.limit(10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_selected[0].asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_selected = [x.asDict()['base_symbol'] for x in assets_selected]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf\n",
    "def check_if_out_of_money(option_type, base_price, strike):\n",
    "    if option_type == 'call' and base_price < strike:\n",
    "        return 1\n",
    "    elif option_type == 'call' and base_price >= strike:\n",
    "        return 0\n",
    "    elif option_type == 'put' and base_price > strike:\n",
    "        return 1\n",
    "    elif option_type == 'put' and base_price <= strike:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_add_cols = (\n",
    "    options \n",
    "        .filter(F.col('base_symbol').isin(assets_selected))\n",
    "        .withColumn('date_parsed', F.to_date(F.col('date'), 'MM/dd/yyyy')) \n",
    "        .withColumn('day', F.dayofmonth(F.col('date_parsed'))) \n",
    "        .withColumn('month', F.month(F.col('date_parsed'))) \n",
    "        .withColumn('year', F.year(F.col('date_parsed'))) \n",
    "        .withColumn('exp_date_parsed', F.to_date(F.col('expiration'), 'MM/dd/yyyy')) \n",
    "        .withColumn('days_diff', F.datediff(F.col('exp_date_parsed'), F.col('date_parsed'))) \n",
    "        .withColumn('weeks_diff', F.col('days_diff') / 7) \n",
    "        .withColumn('bid_ask_mean', (F.col('bid') + F.col('ask')) / 2) \n",
    "        .withColumn('is_call_option', (F.col('type') == 'call').cast(IntegerType())) \n",
    "        .withColumn('strike_over_base', F.col('strike') / F.col('base_price')) \n",
    "        .withColumn(\n",
    "            'out_of_money', check_if_out_of_money(\n",
    "                F.col('type'),\n",
    "                F.col('base_price'),\n",
    "                F.col('strike')\n",
    "            ).cast(IntegerType())\n",
    "        )\n",
    "        .drop('date', 'expiration', 'aka') \n",
    "        .withColumnRenamed('exp_date_parsed', 'expiration_date') \n",
    "        .withColumnRenamed('date_parsed', 'date') \n",
    "        .select(\n",
    "            'base_symbol',\n",
    "            'base_price',\n",
    "            'option_symbol',\n",
    "            'type',\n",
    "            'is_call_option',\n",
    "            'date',\n",
    "            'expiration_date',\n",
    "            'days_diff',\n",
    "            'bid_ask_mean',\n",
    "            'strike',\n",
    "            'strike_over_base',\n",
    "            'out_of_money',\n",
    "            'volume',\n",
    "        )\n",
    "        .orderBy('date')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_add_cols.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_add_cols.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Volatilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_data = (\n",
    "    options\n",
    "        .select(\n",
    "            'base_symbol',\n",
    "            'base_price',\n",
    "            'date'\n",
    "        )\n",
    "        .withColumn('date_parsed', F.to_date(F.col('date'), 'MM/dd/yyyy'))\n",
    "        .drop('date')\n",
    "        .withColumnRenamed('date_parsed', 'date')\n",
    "        .groupBy('base_symbol', 'date')\n",
    "        .agg(\n",
    "             F.first('base_price').alias('base_price')\n",
    "        )\n",
    "        .orderBy('date')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_data.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timestamp is interpreted as UNIX timestamp in seconds\n",
    "days = lambda x: x * 86400 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = (Window()\n",
    "      .partitionBy(F.col('base_symbol'))\n",
    "      .orderBy(F.col('date').cast('timestamp').cast('long'))\n",
    "      .rangeBetween(-days(1 + 1), -days(1)))\n",
    "d2 = (Window()\n",
    "      .partitionBy(F.col('base_symbol'))\n",
    "      .orderBy(F.col('date').cast('timestamp').cast('long'))\n",
    "      .rangeBetween(-days(2 + 1), -days(1)))\n",
    "d3 = (Window()\n",
    "      .partitionBy(F.col('base_symbol'))\n",
    "      .orderBy(F.col('date').cast('timestamp').cast('long'))\n",
    "      .rangeBetween(-days(3 + 1), -days(1)))\n",
    "w1 = (Window()\n",
    "      .partitionBy(F.col('base_symbol'))\n",
    "      .orderBy(F.col('date').cast('timestamp').cast('long'))\n",
    "      .rangeBetween(-days(7 + 1), -days(1)))\n",
    "w2 = (Window()\n",
    "      .partitionBy(F.col('base_symbol'))\n",
    "      .orderBy(F.col('date').cast('timestamp').cast('long'))\n",
    "      .rangeBetween(-days(2 * 7 + 1), -days(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_volatilities = (\n",
    "    stocks_data\n",
    "        .withColumn('1d_mean', F.mean('base_price').over(d1))\n",
    "        .withColumn('2d_mean', F.mean('base_price').over(d2))\n",
    "        .withColumn('3d_mean', F.mean('base_price').over(d3))\n",
    "        .withColumn('1w_mean', F.mean('base_price').over(w1))\n",
    "        .withColumn('2w_mean', F.mean('base_price').over(w2))\n",
    "        .withColumn('1d_std', F.stddev('base_price').over(d1))\n",
    "        .withColumn('2d_std', F.stddev('base_price').over(d2))\n",
    "        .withColumn('3d_std', F.stddev('base_price').over(d3))\n",
    "        .withColumn('1w_std', F.stddev('base_price').over(w1))\n",
    "        .withColumn('2w_std', F.stddev('base_price').over(w2))\n",
    "        .withColumn('1d_volatility', F.col('1d_std') / F.col('1d_mean'))\n",
    "        .withColumn('2d_volatility', F.col('2d_std') / F.col('2d_mean'))\n",
    "        .withColumn('3d_volatility', F.col('3d_std') / F.col('3d_mean'))\n",
    "        .withColumn('1w_volatility', F.col('1w_std') / F.col('1w_mean'))\n",
    "        .withColumn('2w_volatility', F.col('2w_std') / F.col('2w_mean'))\n",
    "        .select(\n",
    "            'base_symbol',\n",
    "            'date',\n",
    "            '1d_mean',\n",
    "            '2d_mean',\n",
    "            '3d_mean',\n",
    "            '1w_mean',\n",
    "            '2w_mean',\n",
    "            '1d_volatility',\n",
    "            '2d_volatility',\n",
    "            '3d_volatility',\n",
    "            '1w_volatility',\n",
    "            '2w_volatility'\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_volatilities.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = stocks_volatilities.agg({'date': 'min'}).collect()[0].asDict()['min(date)']\n",
    "max_date = stocks_volatilities.agg({'date': 'max'}).collect()[0].asDict()['max(date)']\n",
    "print(min_date, max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = min_date + datetime.timedelta(weeks=2) #datetime.date(2017, 1, 1)\n",
    "end_date = max_date\n",
    "\n",
    "features = (\n",
    "    options_add_cols.join(stocks_volatilities, on=['base_symbol', 'date'], how='left')\n",
    "        .filter(F.col('date') > start_date)\n",
    "        .filter(F.col('date') <= end_date)\n",
    "        .orderBy('date')\n",
    "        .drop('option_symbol', 'expiration_date', 'type', 'date')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1. Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.toPandas()\n",
    "features.to_csv('features.csv')\n",
    "features.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2. External data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets = pd.read_csv('../../__OPTIONS/Sector_Industry_Country_MarketCap.csv')\n",
    "markets = markets.rename(columns={'Ticker': 'base_symbol'})\n",
    "markets = markets[[\"base_symbol\",\"Sector\", \"Industry\", \"Country\"]].copy()\n",
    "print(markets.shape)\n",
    "markets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(markets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets_one_hot = pd.get_dummies(markets[['Sector', 'Country']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets = markets.join(markets_one_hot)\n",
    "print(markets.shape)\n",
    "markets.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(markets.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3. Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.join(\n",
    "    markets.set_index('base_symbol'),\n",
    "    on=['base_symbol'], \n",
    "    how='left'\n",
    ")\n",
    "print(features.shape)\n",
    "display(features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.dropna()\n",
    "print(features.shape)\n",
    "display(features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['Sector'].groupby(features['Sector']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markets['Country'].groupby(markets['Country']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features.drop([\n",
    "    'base_symbol',\n",
    "    'Sector',\n",
    "    'Industry',\n",
    "    'Country'], axis=1)\n",
    "print(features.shape)\n",
    "display(features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4. Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = 'bid_ask_mean'\n",
    "feats_cols = [x for x in features.columns if x not in target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features[feats_cols], \n",
    "    features[target_col], \n",
    "    test_size=.3, \n",
    "    random_state=2022\n",
    ")\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next topic..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
